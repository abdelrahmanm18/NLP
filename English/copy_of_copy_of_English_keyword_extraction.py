# -*- coding: utf-8 -*-
"""Copy of Copy of keyword extraction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MSAG7L2UGQ4JS54VGtjT6uJipBseUySN
"""

!pip install  PyPDF2
!pip install yake
!pip install pikepdf

"""**import libraries**"""

import numpy as np
import os
import nltk
from nltk.stem import PorterStemmer
from nltk.tokenize import TweetTokenizer
from natsort import natsorted
import string
import nltk 
nltk.download("punkt")
import re
from nltk.corpus import stopwords
import string
nltk.download('stopwords')
from nltk.tokenize import word_tokenize
from gensim.parsing.preprocessing import remove_stopwords
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
import PyPDF2
import random
from sklearn.decomposition import LatentDirichletAllocation
import pandas as pd 
from sklearn.feature_extraction.text import TfidfVectorizer
import yake
from gensim.summarization import keywords
import spacy
from nltk.stem import PorterStemmer,WordNetLemmatizer 
nltk.download('wordnet')
nltk.download('omw-1.4')
import pikepdf

"""**preprocessing**"""

def preprocessing(final_string):
  t=""
  arr=[]
  for final_string in final_string:
    punc = set(string.punctuation)
    stop_words = set(stopwords.words('english'))
    final_string = re.sub("\w*\d\w*", "", final_string)
    final_string=re.sub(r"\n", "", final_string)
    final_string = final_string.translate (str.maketrans ('', '', string.punctuation)) 
    final_string=final_string.lower()
    cachedStopWords = stopwords.words("english")
    pattern = re.compile(r'\b(' + r'|'.join(cachedStopWords) + r')\b\s*')
    final_string = pattern.sub('', final_string)
    final_string = final_string[:final_string.find('references')].strip()
    Lemma = WordNetLemmatizer()
    final_string=Lemma.lemmatize(final_string)
    arr.append(final_string)
    
  return  arr

"""**read file**"""

def readpdf(pdfpath):
    t=" "
    path = open(pdfpath, 'rb')
    pdfReader = PyPDF2.PdfFileReader(path)
    totalpages = pdfReader.numPages
    for i in range(totalpages):
        from_page = pdfReader.getPage(i)
        text = from_page.extractText()
        t=t+text
    txt=t
    return txt

pdf1=readpdf('/content/JVI.01774-14.pdf')
pdf2=readpdf('/content/2020.emnlp-demos.6.pdf')
pdf3=readpdf('/content/1701.06410v1.pdf')

text_aupdated=preprocessing([pdf1,pdf2,pdf3])

"""**list of preprocessed files**"""

text_aupdated

"""**count vectorizer**"""

cv=CountVectorizer()
word_count_vectr=cv.fit_transform(text_aupdated).toarray()

print(len(cv.get_feature_names()))

pd.DataFrame(word_count_vectr,columns=cv.get_feature_names())

"""**Latent Dirichlet Allocation**"""

LDA = LatentDirichletAllocation(n_components=5,random_state=42)
LDA.fit(word_count_vectr)
topic_results = LDA.transform(word_count_vectr)

"""**show each topic with their words**"""

for index,topic in enumerate(LDA.components_):
    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')
    print([cv.get_feature_names()[i] for i in topic.argsort()[-15:]])
    print('\n')

topic_results.argmax(axis=1)

data_f=pd.DataFrame()
data_f["Articles"]=['pdf1','pdf2','pdf3']
data_f["topic"]=topic_results.argmax(axis=1)
arr=[]
for j in range(len(data_f)):
    arr.append(([cv.get_feature_names()[i] for i in LDA.components_[data_f["topic"][j]].argsort()[-10:]]))
data_f["keywords"]=arr

data_f

"""**return keywords of a specific topic and document**"""

def LDAkeyword(topic_numb,top_keywords_num):

    return [cv.get_feature_names()[i] for i in LDA.components_[topic_numb].argsort()[-top_keywords_num:]]
pd.DataFrame(LDAkeyword(4,20),columns=["doc keywords"] )

"""**TF-IDF**"""

tfidf_vectorizer=TfidfVectorizer(smooth_idf=True,use_idf=True)
tfidf=tfidf_vectorizer.fit_transform(text_aupdated).toarray()
dt_tfidf = pd.DataFrame(tfidf,columns=cv.get_feature_names())

dt_tfidf

"""**tf-idf keywords**"""

def keyword(doc_index,keyword_num):
  return pd.DataFrame(dt_tfidf.iloc[doc_index,:]).nlargest(keyword_num, doc_index)

keyword(1,10)

!pip install yake
!pip install rake_nltk

"""**keywords with yake**"""

def yakekeyword(doc_index):
  KW=[]
  # Input Text
  text = text_aupdated[doc_index]
  # Specifying Parameters
  language = "en"
  max_ngram_size = 3
  deduplication_thresold = 0.9
  deduplication_algo = 'seqm'
  windowSize = 1
  numOfKeywords = 3

  custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_thresold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)
  keywords = custom_kw_extractor.extract_keywords(text)

  for kw,kv in keywords:
      KW.append(kw)
  return KW

"""**keywords with gensim**"""

def gensimkeyword(doc_index):
    text = text_aupdated[doc_index]
    return keywords(text)

"""**keywords with spacy**"""

def spacykeyword(doc_index):
    nlp = spacy.load("en_core_web_sm")
    text = text_aupdated[doc_index]
    doc = nlp(text)
    return (doc.ents)

dataframe=pd.DataFrame()
dataframe['pdf']=['pdf1','pdf2','pdf3']
dataframe['yake keyword']=[yakekeyword(0),yakekeyword(1),yakekeyword(2)]
dataframe['gensim keyword']=[gensimkeyword(0),gensimkeyword(1),gensimkeyword(2)]
dataframe['spacy keyword']=[spacykeyword(0),spacykeyword(1),spacykeyword(2)]

dataframe

"""**extract metadata**"""

def metadata(filepath):
    pdf = pikepdf.Pdf.open(filepath)

    pdf_info = pdf.docinfo

    for key, value in pdf_info.items():
        print(key, ':', value)

metadata('/content/JVI.01774-14.pdf')

"""**arabic data**"""

